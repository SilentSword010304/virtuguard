#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import itertools
from sklearn.metrics import classification_report,confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb
from lightgbm import LGBMClassifier
import os
import seaborn as sns
from wordcloud import WordCloud


# In[2]:


df=pd.read_csv('malicious_phish.csv.zip')

print(df.shape)
df.head()


# In[3]:


df.type.value_counts()


# In[4]:


#Data Filtering:
#Filter the DataFrame df into four separate DataFrames based on the type column: phishing, malware, defacement, and benign.

#URL Concatenation:
#Concatenate all URLs from each category DataFrame into separate strings.

#Word Cloud Generation:
#Generate a word cloud visualization based on the concatenated URLs for phishing websites using the WordCloud library.
#Customize parameters such as width, height, and colormap.

#Plotting:
#Plot the word cloud visualization using matplotlib.
#Set up the figure size and background color.
#Display the word cloud without axis and with tight layout.


# In[5]:


df_phish = df[df.type=='phishing']
df_malware = df[df.type=='malware']
df_deface = df[df.type=='defacement']
df_benign = df[df.type=='benign']


# In[6]:


phish_url = " ".join(i for i in df_phish.url)
wordcloud = WordCloud(width=1600, height=800,colormap='Paired').generate(phish_url)
plt.figure( figsize=(12,14),facecolor='k')
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()


# In[7]:


malware_url = " ".join(i for i in df_malware.url)
wordcloud = WordCloud(width=1600, height=800,colormap='Paired').generate(malware_url)
plt.figure( figsize=(12,14),facecolor='k')
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()


# In[8]:


benign_url = " ".join(i for i in df_benign.url)
wordcloud = WordCloud(width=1600, height=800,colormap='Paired').generate(benign_url)
plt.figure( figsize=(12,14),facecolor='k')
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()


# In[9]:


#This code snippet defines a function having_ip_address(url) that checks if a URL contains an 
#IP address using a regular expression pattern. It then applies this function to each URL in a 
#DataFrame df['url'] and stores the result in a new column 'use_of_ip'. Here's a more concise description:

#Function Definition: Defines a function having_ip_address(url) to check if a URL contains an IP address 
#    using a regular expression pattern.

#Application to DataFrame: Applies the having_ip_address() function to each URL in the DataFrame df['url']
#    using the .apply() method.

#Result Storage: Stores the result (0 or 1) in a new column 'use_of_ip' in the DataFrame df.

#In summary, this code efficiently identifies whether each URL in the DataFrame contains an IP address and 
#adds this information as a new column in the DataFrame.


# In[10]:


import re
#Use of IP or not in domain
def having_ip_address(url):
    match = re.search(
        '(([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.'
        '([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\/)|'  # IPv4
        '((0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\/)' # IPv4 in hexadecimal
        '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}', url)  # Ipv6
    if match:
        # print match.group()
        return 1
    else:
        # print 'No matching pattern found'
        return 0
df['use_of_ip'] = df['url'].apply(lambda i: having_ip_address(i))


# In[11]:


#This code defines a function abnormal_url(url) that checks if the hostname is present in the URL. 
#It then applies this function to each URL in a DataFrame df['url'] and stores the result in a new 
#column 'abnormal_url'. Here's a more concise description:

#Function Definition: Defines a function abnormal_url(url) to check if the hostname is present in the 
#    URL using urlparse() and a regular expression pattern.

#Application to DataFrame: Applies the abnormal_url() function to each URL in the DataFrame df['url'] 
#    using the .apply() method.

#Result Storage: Stores the result (0 or 1) indicating whether the hostname is present in the URL as a 
#    new column 'abnormal_url' in the DataFrame df.

#In summary, this code efficiently identifies whether each URL in the DataFrame contains its hostname 
#as a substring and adds this information as a new column in the DataFrame.


# In[12]:


from urllib.parse import urlparse

def abnormal_url(url):
    hostname = urlparse(url).hostname
    hostname = str(hostname)
    match = re.search(hostname, url)
    if match:
        # print match.group()
        return 1
    else:
        # print 'No matching pattern found'
        return 0


df['abnormal_url'] = df['url'].apply(lambda i: abnormal_url(i))


# In[13]:


#!pip install googlesearch-python


# In[14]:


from googlesearch import search


# In[15]:


#This code snippet adds two new columns to the DataFrame df:

#Google Index Check (google_index function):

#Defines a function google_index(url) that checks if a URL is indexed by Google.
#It returns 1 if the URL is indexed, 0 otherwise.
#Applies this function to each URL in df['url'] and stores the result in 'google_index'.

#Counting Dots (count_dot function):
#Defines a function count_dot(url) that counts dots in a URL.


# In[16]:


def google_index(url):
    site = search(url, 5)
    return 1 if site else 0
df['google_index'] = df['url'].apply(lambda i: google_index(i))


# In[17]:


def count_dot(url):
    count_dot = url.count('.')
    return count_dot

df['count.'] = df['url'].apply(lambda i: count_dot(i))
df.head()


# In[18]:


def count_www(url):
    url.count('www')
    return url.count('www')

df['count-www'] = df['url'].apply(lambda i: count_www(i))

def count_atrate(url):
     
    return url.count('@')

df['count@'] = df['url'].apply(lambda i: count_atrate(i))


def no_of_dir(url):
    urldir = urlparse(url).path
    return urldir.count('/')

df['count_dir'] = df['url'].apply(lambda i: no_of_dir(i))

def no_of_embed(url):
    urldir = urlparse(url).path
    return urldir.count('//')

df['count_embed_domian'] = df['url'].apply(lambda i: no_of_embed(i))


def shortening_service(url):
    match = re.search('bit\.ly|goo\.gl|shorte\.st|go2l\.ink|x\.co|ow\.ly|t\.co|tinyurl|tr\.im|is\.gd|cli\.gs|'
                      'yfrog\.com|migre\.me|ff\.im|tiny\.cc|url4\.eu|twit\.ac|su\.pr|twurl\.nl|snipurl\.com|'
                      'short\.to|BudURL\.com|ping\.fm|post\.ly|Just\.as|bkite\.com|snipr\.com|fic\.kr|loopt\.us|'
                      'doiop\.com|short\.ie|kl\.am|wp\.me|rubyurl\.com|om\.ly|to\.ly|bit\.do|t\.co|lnkd\.in|'
                      'db\.tt|qr\.ae|adf\.ly|goo\.gl|bitly\.com|cur\.lv|tinyurl\.com|ow\.ly|bit\.ly|ity\.im|'
                      'q\.gs|is\.gd|po\.st|bc\.vc|twitthis\.com|u\.to|j\.mp|buzurl\.com|cutt\.us|u\.bb|yourls\.org|'
                      'x\.co|prettylinkpro\.com|scrnch\.me|filoops\.info|vzturl\.com|qr\.net|1url\.com|tweez\.me|v\.gd|'
                      'tr\.im|link\.zip\.net',
                      url)
    if match:
        return 1
    else:
        return 0
    
    
df['short_url'] = df['url'].apply(lambda i: shortening_service(i))


# In[19]:


def count_https(url):
    return url.count('https')

df['count-https'] = df['url'].apply(lambda i : count_https(i))

def count_http(url):
    return url.count('http')

df['count-http'] = df['url'].apply(lambda i : count_http(i))


# In[20]:


def count_per(url):
    return url.count('%')

df['count%'] = df['url'].apply(lambda i : count_per(i))

def count_ques(url):
    return url.count('?')

df['count?'] = df['url'].apply(lambda i: count_ques(i))

def count_hyphen(url):
    return url.count('-')

df['count-'] = df['url'].apply(lambda i: count_hyphen(i))

def count_equal(url):
    return url.count('=')

df['count='] = df['url'].apply(lambda i: count_equal(i))

def url_length(url):
    return len(str(url))


#Length of URL
df['url_length'] = df['url'].apply(lambda i: url_length(i))
#Hostname Length

def hostname_length(url):
    return len(urlparse(url).netloc)

df['hostname_length'] = df['url'].apply(lambda i: hostname_length(i))

df.head()

def suspicious_words(url):
    match = re.search('PayPal|login|signin|bank|account|update|free|lucky|service|bonus|ebayisapi|webscr',
                      url)
    if match:
        return 1
    else:
        return 0
df['sus_url'] = df['url'].apply(lambda i: suspicious_words(i))


def digit_count(url):
    digits = 0
    for i in url:
        if i.isnumeric():
            digits = digits + 1
    return digits


df['count-digits']= df['url'].apply(lambda i: digit_count(i))


def letter_count(url):
    letters = 0
    for i in url:
        if i.isalpha():
            letters = letters + 1
    return letters


df['count-letters']= df['url'].apply(lambda i: letter_count(i))

df.head()


# In[21]:


#!pip install tld


# In[22]:


#Importing dependencies
from urllib.parse import urlparse
from tld import get_tld
import os.path

#First Directory Length
def fd_length(url):
    urlpath= urlparse(url).path
    try:
        return len(urlpath.split('/')[1])
    except:
        return 0

df['fd_length'] = df['url'].apply(lambda i: fd_length(i))

#Length of Top Level Domain
df['tld'] = df['url'].apply(lambda i: get_tld(i,fail_silently=True))


def tld_length(tld):
    try:
        return len(tld)
    except:
        return -1

df['tld_length'] = df['tld'].apply(lambda i: tld_length(i))


# In[23]:


df = df.drop("tld",1)


# In[24]:


df.columns


# In[25]:


df['type'].value_counts()


# In[26]:


from sklearn.preprocessing import LabelEncoder

lb_make = LabelEncoder()
df["type_code"] = lb_make.fit_transform(df["type"])
df["type_code"].value_counts()


# In[27]:


#Predictor Variables
# filtering out google_index as it has only 1 value
X = df[['use_of_ip','abnormal_url', 'count.', 'count-www', 'count@',
       'count_dir', 'count_embed_domian', 'short_url', 'count-https',
       'count-http', 'count%', 'count?', 'count-', 'count=', 'url_length',
       'hostname_length', 'sus_url', 'fd_length', 'tld_length', 'count-digits',
       'count-letters']]

#Target Variable
y = df['type_code']


# In[28]:


X.head()


# In[29]:


X.columns


# In[30]:


X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2,shuffle=True, random_state=5)


# In[31]:


# from sklearn import metrics
# from sklearn.metrics import classification_report
# from sklearn.ensemble import RandomForestClassifier
# rf = RandomForestClassifier(n_estimators=100,max_features='sqrt')
# rf.fit(X_train,y_train)
# y_pred_rf = rf.predict(X_test)
# print(classification_report(y_test,y_pred_rf,target_names=['benign', 'defacement','phishing','malware']))

# score = metrics.accuracy_score(y_test, y_pred_rf)
# print("accuracy:   %0.3f" % score)


# In[32]:


pip install lightgbm


# In[36]:


from lightgbm import LGBMClassifier
from sklearn import metrics
from sklearn.metrics import classification_report

lgb = LGBMClassifier(objective='multiclass',boosting_type= 'gbdt',n_jobs = 5, 
          silent = True, random_state=5)
LGB_C = lgb.fit(X_train, y_train)


y_pred_lgb = LGB_C.predict(X_test)
print(classification_report(y_test,y_pred_lgb,target_names=['benign', 'defacement','phishing','malware']))

score = metrics.accuracy_score(y_test, y_pred_lgb)
print("accuracy:   %0.3f" % score)


# In[37]:


# from xgboost import XGBClassifier
# from sklearn import metrics
# from sklearn.metrics import classification_report
# xgb_c = xgb.XGBClassifier(n_estimators= 100)
# xgb_c.fit(X_train,y_train)
# y_pred_x = xgb_c.predict(X_test)
# print(classification_report(y_test,y_pred_x,target_names=['benign', 'defacement','phishing','malware']))


# score = metrics.accuracy_score(y_test, y_pred_x)
# print("accuracy:   %0.3f" % score)


# In[38]:


import random

# Define the antibody representation
def generate_antibody(length):
    return [random.randint(0, 1) for _ in range(length)]

# Calculate the Hamming distance between two antibodies
def hamming_distance(antibody1, antibody2):
    return sum(a != b for a, b in zip(antibody1, antibody2))

# Generate a population of antibodies
def generate_population(population_size, antibody_length):
    return [generate_antibody(antibody_length) for _ in range(population_size)]

# Detect anomalies using negative selection
def negative_selection(data, population, threshold=0.1):
    anomalies = []
    for pattern in data:
        detected = False
        for antibody in population:
            if hamming_distance(pattern, antibody) <= threshold * len(pattern):
                detected = True
                break
        if not detected:
            anomalies.append(pattern)
    return anomalies

# Main function to train and test the model
def main():
    # Your existing code for data preprocessing and model training

    # Generate a population of antibodies
    antibody_length = len(X.columns)
    population_size = 100
    population = generate_population(population_size, antibody_length)

    # Train the model (e.g., using LGBMClassifier)
    lgb = LGBMClassifier(objective='multiclass', boosting_type='gbdt', n_jobs=5, silent=True, random_state=5)
    LGB_C = lgb.fit(X_train, y_train)

    # Test the model
    y_pred_lgb = LGB_C.predict(X_test)

    # Evaluate the model
    print(classification_report(y_test, y_pred_lgb, target_names=['benign', 'defacement', 'phishing', 'malware']))
    score = metrics.accuracy_score(y_test, y_pred_lgb)
    print("accuracy:   %0.3f" % score)

    # Detect anomalies using negative selection
    data = X_train  # You can use the training data for negative selection
    anomalies = negative_selection(data, population)
    print("Detected anomalies:", anomalies)

# Call the main function
if __name__ == "__main__":
    main()


# In[39]:


def main(url):
    
    status = []
    
    status.append(having_ip_address(url))
    status.append(abnormal_url(url))
    status.append(count_dot(url))
    status.append(count_www(url))
    status.append(count_atrate(url))
    status.append(no_of_dir(url))
    status.append(no_of_embed(url))
    
    status.append(shortening_service(url))
    status.append(count_https(url))
    status.append(count_http(url))
    
    status.append(count_per(url))
    status.append(count_ques(url))
    status.append(count_hyphen(url))
    status.append(count_equal(url))
    
    status.append(url_length(url))
    status.append(hostname_length(url))
    status.append(suspicious_words(url))
    status.append(digit_count(url))
    status.append(letter_count(url))
    status.append(fd_length(url))
    tld = get_tld(url,fail_silently=True)
      
    status.append(tld_length(tld))
    
    
    

    return status


# In[40]:


def get_prediction_from_url(test_url):
    features_test = main(test_url)
    # Due to updates to scikit-learn, we now need a 2D array as a parameter to the predict function.
    features_test = np.array(features_test).reshape((1, -1))

    

    pred = lgb.predict(features_test)
    if int(pred[0]) == 0:
        
        res="SAFE"
        return res
    elif int(pred[0]) == 1.0:
        
        res="DEFACEMENT"
        return res
    elif int(pred[0]) == 2.0:
        res="PHISHING"
        return res
        
    elif int(pred[0]) == 3.0:
        
        res="MALWARE"
        return res


# In[41]:


urls = ['br-icloud.com.br','en.wikipedia.org/wiki/North_Dakota']
for url in urls:
     print(get_prediction_from_url(url))


# In[ ]:





# In[ ]:




